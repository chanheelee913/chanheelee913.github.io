---
title: 8장. 이미지를 분류합니다 - 합성곱 신경망
excerpt: Doit!딥러닝입문 8장
category: AI
use_math: true
---
> Do it! 정직하게 코딩하며 배우는 딥러닝 입문을 읽고 정리

### 합성곱 신경망 
---
#### 이미지 처리 시 기존 신경망의 한계
앞에서 구현했던 신경망들의 경우, 이미지를 신경망의 입력으로 사용할 때  
이미지의 2차원 픽셀 데이터를 일렬로 펼쳐서 입력해야 했다.  

이미지는 주변 값들과 상관이 있는 데이터이기 때문에 펼쳐서 넣으면 정보가 사라지게 된다.  
  
#### 합성곱 신경망  
합성곱 신경망은 이러한 문제점을 해결할 수 있는 새로운 신경망으로 등장했다.  

합성곱 신경망은 눈의 수용 필드에서 물체를 인식하고 발화할지의 여부를 결정한다는 것에서 아이디어를 얻었다. 

합성곱 신경망은 이미지를 일렬로 펼치지 않아도 되어, 주변 정보들이 사라지지 않는다.

#### 합성곱과 교차상관
합성곱 신경망에서는 합성곱 연산과 교차상관 연산 둘 중 하나가 사용된다.

예시 데이터로 입력 x와 가중치 배열 w가 주어졌다.  
> x = [1,3,2,2,0,2,3]  
> w = [-1,0,1]

#### 합성곱 연산
합성곱 연산은 두 배열 중 하나를 뒤집은 후 연산을 한다.


#### 교차상관 연산
교차상관 연산은 합성곱 연산처럼 배열을 뒤집지 않고 그대로 연산을 한다.  

#### 주로 사용하는 것은?
사실 합성곱 신경망에서 주로 사용되는 연산은 교차상관 연산이다.

필터의 가중치를 학습시키는 것이 목적이기 때문에 필터를 뒤집는 합성곱 연산을 사용하든,  
뒤집지 않는 교차상관 연산을 사용하든 별 차이가 없기 때문이다.


### 패딩


### 스트라이드

필터의 이동 간격을 조절한다.

### pooling(풀링)층 
합성곱 신경망은 합성곱 연산이 이루어지는 합성곱층이 있고,  
풀링 연산이 이루어지는 풀링층이 있다.  

풀링층을 통과한 특성맵은 크기가 작아지게 된다.

이 책에서는 두 가지 방법의 풀링을 소개한다.   

#### max pooling
max pooling(최대 풀링)은 풀링 영역에서 최댓값을 선택하는 방식이다.  

#### average pooling
average pooling(평균 풀링)은 풀링 영역 값들의 평균을 계산하는 방식이다.  

### ReLU 함수
합성곱 신경망에서는 시그모이드형 함수 대신 ReLU함수를 활성화 함수로 주로 사용한다.  

ReLU 함수의 모양은 다음 이미지와 같다.

![ReLU](/assets/images/pages/AI/ReLU.jpg)

식으로 표현하면,
  
$$
y=
\begin{cases}
0, & x \le 0 \\
x, & x > 0 \\
\end{cases}
$$

입력값이 0보다 작거나 같은 경우에는 0을, 0보다 큰 경우에는 입력값 자체를 내보내는 함수이다. 

#### ReLU 함수의 장점
시그모이드 함수와 같은 s자 함수의 경우, 기울기가 역전파되면서 값이 점점 작아진다는 특징을 가지고 있다.  
문제는 값이 점점 더 작아지면 연산에 필요한 시간이 늘어나면서 결국 학습 속도에 영향을 주게 되는 것이다.  
이를 기울기 소멸(gradient vanishing)문제라고 한다.  

ReLU함수는 기울기 소멸 문제를 줄여준다.  
0보다 작은 값은 0을 내뷰내기 때문이다.  

### Dropout(드롭아웃)층
Dropout은 모델 학습 시 유닛의 일부를 랜덤하게 학습에 참여시키지 않는 방법을 말한다.  
Dropout 기법을 사용하여 모델이 과적합되는 것을 방지할 수 있다고 알려져 있다. 

keras의 layers 모듈에는 드롭아웃을 쉽게 구현할 수 있는 Dropout 클래스가 구현되어 있다.

~~~python
from tensorflow.keras.layers import Dropout

Dropout((rate))
#rate에는 드롭아웃될 유닛의 비율을 입력한다. (0~1사이의 값)
~~~

### Adam 옵티마이저

새로 소개된 adam 옵티마이저는  
학습률이 전역 최저값에 수렴할수록 학습률을 점점 줄여준다. 