---
title: 9장. 텍스트를 분류합니다 - 순환 신경망
excerpt: Doit!딥러닝입문 9장
category: AI
use_math: true
---
> Do it! 정직하게 코딩하며 배우는 딥러닝 입문을 읽고 정리

### 순환 신경망 
---
#### 순차 데이터 

우리 일상속의 데이터들은 서로 연관이 있는 경우가 많다.  
날씨 데이터, 주식 데이터 등등은 일정한 시간에 따라 연관이 있다.  

일정한 시간 간격대로 배치되어 있는 데이터들을 **시계열 데이터 (time series data)** 라고 한다.

시계열 데이터를 포함하여 순서가 정해져 있는 데이터를 **순차 데이터 (sequential data)**라고 한다.  
그리고 모델이 순차 데이터를 처리하는 순서를 **타임 스텝(time step)** 이라고 한다.

#### 순환 신경망의 등장 배경
순차 데이터들을 잘 처리하기 위해서는, 모델이 이전 데이터들에 대한 정보를 유지하고 있어야 한다.

그러나 완전 연결 신경망이나 합성곱 신경망은 샘플 처리 시 이전 데이터에 대한 정보를 가지고 있지 않으며,  
그로 인해 샘플 처리에 이전 샘플에 대한 영향을 주지 못한다.  

이러한 단점을 해결하기 위해 **순환 신경망 (Recurrent Neural Network, RNN)**을 사용하기 시작했다.

#### 순환신경망의 구조
![rnn](/assets/images/pages/AI/RNN.png)
  
RNN에서 순환 구조를 가지는 은닉층을 **순환층**이라고 한다.  
순환층의 출력은 다시 입력으로 사용되어, 현재의 샘플을 처리할 때 이전 샘플의 정보를 사용할 수 있다.  

#### 셀 (cell)
순환 신경망에서의 층이나 유닛을 말한다.  
셀의 출력을 hidden state라고 부른다.  

![cell](/assets/images/pages/AI/cell.png)

$X$는 입력, $H$는 셀의 출력을 의미하고,  
$H_p$는 이전 타임스텝에서 셀의 출력을 의미한다.  

순환 신경망의 활성화 함수는 하이퍼볼릭 탄젠트($tanh$) 함수이다.

### LSTM
---
#### RNN의 한계와 LSTM의 개발

순환신경망은 이전 타임스텝에서 처리한 데이터의 정보를 유지할 수 있어,  
현재 타임스텝에서 데이터를 처리할 때 이전 타임스탭의 데이터를 고려할 수 있다는 장점이 있다.  

하지만 멀리 떨어진 타임스텝의 경우에는 순환신경망이 계산하며 정보를 유지하기 어렵다.  
계산할수록 그레이디언트 값이 낮아지는 그레이디언트 소멸 문제가 나타나기 때문이다.  

그레이디언트 소멸 문제를 해결하기 위해서 LSTM 순환신경망이 고안되었다.  
LSTM은 멀리 떨어져 있는 타임스텝의 데이터를 처리할 수 있다.  

#### LSTM의 셀 구조
![lstm cell 구조](/assets/images/pages/AI/LSTM.png)

- $C$는 셀 상태로, 셀로만 순환되는 출력이다.  

셀의 상세 구조에서의 식을 살펴보면,  
- $F = {Sigmoid(Z_f)} \times {C_p}$ : 삭제 게이트 (forget gate)
- $I = {Sigmoid(Z_i)} \times {tanh(Z_j)}$ : 입력 게이트 (input gate)
- $C = F+I$
- $H = {tanh(C)} \times {Sigmoid(Z_o)}$ : 출력 게이트 (output gate)

구조는 꽤 복잡하지만, 텐서플로를 사용하면 쉽게 구현할 수 있다.  

#### Tensorflow의 LSTM 신경망

keras의 LSTM 모델을 활용하여 쉽게 LSTM을 구현할 수 있다.  
LSTM 신경망은 keras의 layers 모듈에 위치해 있다.  

~~~python
from tensorflow.keras.layers import LSTM

model = Sequential()

model.add(LSTM(8))

~~~