---
title: 3장. 머신러닝의 기초를 다집니다 - 수치 예측
excerpt: Doit!딥러닝입문 3장
category: AI
---
> Do it! 정직하게 코딩하며 배우는 딥러닝 입문을 읽고 정리

### 선형 회귀 (Linear Regression)
입력 데이터($x$)와 타깃 데이터($y$)를 통해 
직선의 기울기($a$)와 절편($b$)를 찾는 것을 말한다.  

데이터들의 산점도가 주어졌을 때, 데이터들의 분포를 적절하게 표현하는
직선의 방정식을 찾는 것이다.  

### 경사하강법 (Gradient Descent)
학습 모델이 데이터를 적절하게 표현하기 위해 기울기를 조절하는 최적화 알고리즘이다.  
= 어떤 손실함수가 정의되었을 때, 손실함수의 값이 최소가 되는 지점을 찾는 방법.  

$$\hat{y}=wx+b$$

- $\hat{y}$는 모델의 예측값을 의미한다.  
- $w$는 모델의 기울기를 의미한다. ($\theta$로 표기하는 경우도 있다.)
- $b$는 모델의 절편을 의미한다.  

<span style="font-size:80%">*회귀 문제를 해결하는 방법에는 정규방정식, 결정트리, svm등 다양한 방법이 있다.</span>

- 모델이 적절한 기울기를 찾는 방법
    1. 무작위의 w와 b의 값으로 초기화한다.  
    2. 데이터를 입력하여 $\hat{y}$ 를 얻는다.
    3. 예측값인 $\hat{y}$와 실제값인 $y$의 차이를 비교한다.  
    4. 두 값의 차이가 작아지는 방향으로 w와 b를 업데이트한다.  
    5. 모든 데이터를 학습에 사용할 때까지 2~4를 반복한다.  

- 기울기를 업데이트 하는 방법
변화율이 양수일 때는 w값을 증가 시 y 증가.  
변화율이 음수일 때는 w값 감소 시 y 증가.

단순하게 w의 변화율을 더하면 됨.

### 제곱 오차 (squared error)
모델의 예측값 ($\hat{y}$)과 타깃값($y$)를 뺀 후 제곱한 함수이다.  


$$SE=(y-\hat{y})^2$$

- 제곱오차를 가중치에 대해 미분

$$

$$

$$

$$

파이썬 코드로는
~~~python
w = w+ w_grad
~~~


### 그레이디언트 (gradient)
손실함수의 변화율. 다른 책에서는 **경사** 라고도 표기된다.  

